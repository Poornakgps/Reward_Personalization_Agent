{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Timing Optimization\n",
    "\n",
    "This notebook implements algorithms to optimize email delivery timing based on customer engagement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, time\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pytz\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load events data (which contains email opens and clicks)\n",
    "with open('../data/events.json', 'r') as f:\n",
    "    events = json.load(f)\n",
    "    \n",
    "# Load customer data\n",
    "with open('../data/customers.json', 'r') as f:\n",
    "    customers = json.load(f)\n",
    "    \n",
    "# Convert to DataFrames\n",
    "events_df = pd.DataFrame(events)\n",
    "customers_df = pd.DataFrame(customers)\n",
    "\n",
    "print(f\"Loaded {len(events_df)} events and {len(customers_df)} customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filter events for email-related activities\n",
    "email_events = events_df[events_df['event_type'].isin(['email_open', 'email_click'])].copy()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "email_events['timestamp'] = pd.to_datetime(email_events['timestamp'])\n",
    "\n",
    "# Extract time-related features\n",
    "email_events['hour'] = email_events['timestamp'].dt.hour\n",
    "email_events['day_of_week'] = email_events['timestamp'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "email_events['is_weekend'] = email_events['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Extract email_id from metadata\n",
    "email_events['email_id'] = email_events['metadata'].apply(lambda x: x.get('email_id', None))\n",
    "email_events['campaign_id'] = email_events['metadata'].apply(lambda x: x.get('campaign_id', None))\n",
    "\n",
    "print(f\"Found {len(email_events)} email-related events\")\n",
    "email_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Engagement Patterns by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze engagement by hour of day\n",
    "hourly_engagement = email_events.groupby(['hour', 'event_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate open and click rates by hour\n",
    "if 'email_open' in hourly_engagement.columns and 'email_click' in hourly_engagement.columns:\n",
    "    hourly_engagement['click_to_open_ratio'] = hourly_engagement['email_click'] / hourly_engagement['email_open']\n",
    "    hourly_engagement['click_to_open_ratio'] = hourly_engagement['click_to_open_ratio'].fillna(0)\n",
    "\n",
    "# Plot hourly engagement\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create plot with two y-axes\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot open and click counts on left y-axis\n",
    "if 'email_open' in hourly_engagement.columns:\n",
    "    ax1.plot(hourly_engagement.index, hourly_engagement['email_open'], 'o-', color='blue', label='Opens')\n",
    "if 'email_click' in hourly_engagement.columns:\n",
    "    ax1.plot(hourly_engagement.index, hourly_engagement['email_click'], 'o-', color='green', label='Clicks')\n",
    "\n",
    "# Plot click-to-open ratio on right y-axis\n",
    "if 'click_to_open_ratio' in hourly_engagement.columns:\n",
    "    ax2.plot(hourly_engagement.index, hourly_engagement['click_to_open_ratio'], 'o--', color='red', label='Click/Open Ratio')\n",
    "\n",
    "ax1.set_xlabel('Hour of Day')\n",
    "ax1.set_ylabel('Count')\n",
    "ax2.set_ylabel('Click-to-Open Ratio')\n",
    "ax1.set_title('Email Engagement by Hour of Day')\n",
    "ax1.set_xticks(range(24))\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze engagement by day of week\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_engagement = email_events.groupby(['day_of_week', 'event_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate open and click rates by day\n",
    "if 'email_open' in daily_engagement.columns and 'email_click' in daily_engagement.columns:\n",
    "    daily_engagement['click_to_open_ratio'] = daily_engagement['email_click'] / daily_engagement['email_open']\n",
    "    daily_engagement['click_to_open_ratio'] = daily_engagement['click_to_open_ratio'].fillna(0)\n",
    "\n",
    "# Prepare data for plotting with day names\n",
    "daily_engagement = daily_engagement.reset_index()\n",
    "daily_engagement['day_name'] = daily_engagement['day_of_week'].apply(lambda x: day_names[x])\n",
    "daily_engagement = daily_engagement.set_index('day_name')\n",
    "daily_engagement = daily_engagement.reindex(day_names)  # Ensure correct order\n",
    "daily_engagement = daily_engagement.drop('day_of_week', axis=1)\n",
    "\n",
    "# Plot daily engagement\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create plot with two y-axes\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot open and click counts on left y-axis\n",
    "if 'email_open' in daily_engagement.columns:\n",
    "    ax1.bar(daily_engagement.index, daily_engagement['email_open'], width=0.4, alpha=0.7, color='blue', label='Opens')\n",
    "if 'email_click' in daily_engagement.columns:\n",
    "    ax1.bar(daily_engagement.index, daily_engagement['email_click'], width=0.4, alpha=0.7, color='green', label='Clicks')\n",
    "\n",
    "# Plot click-to-open ratio on right y-axis\n",
    "if 'click_to_open_ratio' in daily_engagement.columns:\n",
    "    ax2.plot(daily_engagement.index, daily_engagement['click_to_open_ratio'], 'o--', color='red', label='Click/Open Ratio', linewidth=2, markersize=10)\n",
    "\n",
    "ax1.set_xlabel('Day of Week')\n",
    "ax1.set_ylabel('Count')\n",
    "ax2.set_ylabel('Click-to-Open Ratio')\n",
    "ax1.set_title('Email Engagement by Day of Week')\n",
    "ax1.set_xticklabels(daily_engagement.index, rotation=45)\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Customer Timing Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze individual customer patterns\n",
    "customer_timing = email_events.groupby(['customer_id', 'hour']).size().unstack(fill_value=0)\n",
    "\n",
    "# Normalize by customer to get engagement probability distribution\n",
    "customer_timing_norm = customer_timing.div(customer_timing.sum(axis=1), axis=0)\n",
    "\n",
    "# For visualization, select a few random customers\n",
    "sample_customers = np.random.choice(customer_timing_norm.index, min(5, len(customer_timing_norm)), replace=False)\n",
    "customer_samples = customer_timing_norm.loc[sample_customers]\n",
    "\n",
    "# Plot customer timing patterns\n",
    "plt.figure(figsize=(14, 7))\n",
    "for customer_id in customer_samples.index:\n",
    "    plt.plot(customer_samples.columns, customer_samples.loc[customer_id], 'o-', label=f\"Customer {customer_id}\")\n",
    "\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Normalized Engagement')\n",
    "plt.title('Email Engagement Patterns by Customer')\n",
    "plt.xticks(range(24))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cluster customers by timing patterns\n",
    "# Prepare data for clustering\n",
    "X = customer_timing_norm.fillna(0).values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), wcss, 'o-')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply K-means clustering with optimal k (e.g., k=4)\n",
    "optimal_k = 4  # Choose based on elbow plot\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "customer_timing_norm['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Analyze cluster timing patterns\n",
    "cluster_timing = customer_timing_norm.groupby('cluster').mean()\n",
    "\n",
    "# Convert to DataFrame without cluster column\n",
    "cluster_timing_df = cluster_timing.loc[:, [col for col in cluster_timing.columns if col != 'cluster']]\n",
    "\n",
    "# Plot cluster timing patterns\n",
    "plt.figure(figsize=(14, 7))\n",
    "for cluster_id in cluster_timing_df.index:\n",
    "    plt.plot(cluster_timing_df.columns, cluster_timing_df.loc[cluster_id], 'o-', \n",
    "             label=f\"Cluster {cluster_id}\", linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Normalized Engagement')\n",
    "plt.title('Email Engagement Patterns by Customer Cluster')\n",
    "plt.xticks(range(24))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Name clusters based on timing patterns\n",
    "cluster_names = {\n",
    "    0: \"Morning Engagers\",   # Peak in morning hours\n",
    "    1: \"Afternoon Engagers\",  # Peak in afternoon hours\n",
    "    2: \"Evening Engagers\",    # Peak in evening hours\n",
    "    3: \"Night Owls\"           # Peak late at night\n",
    "}\n",
    "\n",
    "# Determine peak hours for each cluster\n",
    "peak_hours = {}\n",
    "for cluster_id in cluster_timing_df.index:\n",
    "    cluster_data = cluster_timing_df.loc[cluster_id]\n",
    "    peak_hour = cluster_data.idxmax()\n",
    "    peak_hours[cluster_id] = peak_hour\n",
    "\n",
    "# Add cluster information to customers\n",
    "customer_clusters = customer_timing_norm[['cluster']].copy()\n",
    "customer_clusters['cluster_name'] = customer_clusters['cluster'].map(cluster_names)\n",
    "customer_clusters['peak_hour'] = customer_clusters['cluster'].map(peak_hours)\n",
    "\n",
    "# Count customers in each cluster\n",
    "cluster_counts = customer_clusters['cluster_name'].value_counts()\n",
    "\n",
    "# Plot distribution of customers across clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "cluster_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Customers by Engagement Time Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Optimal Send Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Merge customer clusters with customer attributes\n",
    "# Extract customer attributes\n",
    "def extract_customer_attributes(customer):\n",
    "    attributes = {}\n",
    "    attributes['id'] = customer.get('id')\n",
    "    \n",
    "    # Extract demographic attributes\n",
    "    customer_attrs = customer.get('attributes', {})\n",
    "    attributes['age'] = customer_attrs.get('age', 0)\n",
    "    attributes['gender'] = customer_attrs.get('gender', 'unknown')\n",
    "    attributes['location'] = customer_attrs.get('location', 'unknown')\n",
    "    attributes['interests'] = ','.join(customer_attrs.get('interests', []))\n",
    "    \n",
    "    return attributes\n",
    "\n",
    "# Create customer attributes DataFrame\n",
    "customer_attrs_list = [extract_customer_attributes(customer) for customer in customers]\n",
    "customer_attrs_df = pd.DataFrame(customer_attrs_list)\n",
    "customer_attrs_df.set_index('id', inplace=True)\n",
    "\n",
    "# Merge with cluster information\n",
    "customer_data = pd.merge(\n",
    "    customer_attrs_df, \n",
    "    customer_clusters, \n",
    "    left_index=True, \n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing cluster information for customers without engagement data\n",
    "customer_data['cluster'] = customer_data['cluster'].fillna(-1).astype(int)\n",
    "customer_data['cluster_name'] = customer_data['cluster_name'].fillna('Unknown')\n",
    "customer_data['peak_hour'] = customer_data['peak_hour'].fillna(12)  # Default to noon\n",
    "\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for training send time prediction model\n",
    "# Only use customers with known clusters\n",
    "model_data = customer_data[customer_data['cluster'] >= 0].copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "model_data_encoded = pd.get_dummies(\n",
    "    model_data, \n",
    "    columns=['gender', 'location'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# Create binary interest indicators\n",
    "common_interests = ['fashion', 'technology', 'sports', 'beauty', 'home', 'travel', 'food']\n",
    "for interest in common_interests:\n",
    "    model_data_encoded[f'interest_{interest}'] = model_data_encoded['interests'].str.contains(interest).astype(int)\n",
    "\n",
    "# Split features and target\n",
    "X = model_data_encoded.drop(['cluster', 'cluster_name', 'peak_hour', 'interests'], axis=1)\n",
    "y = model_data_encoded['peak_hour']\n",
    "\n",
    "# Train a Random Forest model to predict optimal send hour\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10), palette='viridis')\n",
    "plt.title('Top 10 Features for Predicting Optimal Send Time')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to predict optimal send time for a customer\n",
    "def predict_optimal_send_time(customer_id, customer_data, rf_model, X_columns):\n",
    "    \"\"\"Predict optimal send time for a customer.\"\"\"\n",
    "    if customer_id not in customer_data.index:\n",
    "        return 12  # Default to noon if customer not found\n",
    "        \n",
    "    # Get customer data\n",
    "    customer = customer_data.loc[customer_id]\n",
    "    \n",
    "    # If customer has an assigned cluster, use peak hour\n",
    "    if customer['cluster'] >= 0:\n",
    "        return customer['peak_hour']\n",
    "        \n",
    "    # Otherwise, use the model to predict\n",
    "    # Prepare customer features\n",
    "    customer_features = {}\n",
    "    \n",
    "    # Add numeric features\n",
    "    customer_features['age'] = customer['age']\n",
    "    \n",
    "    # Add encoded categorical features\n",
    "    for column in X_columns:\n",
    "        if column.startswith('gender_') or column.startswith('location_'):\n",
    "            # Check if column would be 1 for this customer\n",
    "            if column == f\"gender_{customer['gender']}\" or column == f\"location_{customer['location']}\":\n",
    "                customer_features[column] = 1\n",
    "            else:\n",
    "                customer_features[column] = 0\n",
    "                \n",
    "    # Add interest indicators\n",
    "    interests = customer['interests'].split(',')\n",
    "    for column in X_columns:\n",
    "        if column.startswith('interest_'):\n",
    "            interest = column.replace('interest_', '')\n",
    "            customer_features[column] = 1 if interest in interests else 0\n",
    "    \n",
    "    # Create feature array in correct order\n",
    "    X_customer = pd.DataFrame([customer_features], columns=X_columns)\n",
    "    \n",
    "    # Fill any missing columns with 0\n",
    "    for column in X_columns:\n",
    "        if column not in X_customer.columns:\n",
    "            X_customer[column] = 0\n",
    "    \n",
    "    # Ensure columns are in the right order\n",
    "    X_customer = X_customer[X_columns]\n",
    "    \n",
    "    # Predict optimal hour\n",
    "    predicted_hour = rf_model.predict(X_customer)[0]\n",
    "    \n",
    "    # Round to nearest hour\n",
    "    return round(predicted_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the prediction function with a few customers\n",
    "test_customers = np.random.choice(customer_data.index, 5)\n",
    "\n",
    "for customer_id in test_customers:\n",
    "    optimal_hour = predict_optimal_send_time(customer_id, customer_data, rf_model, X.columns)\n",
    "    cluster_name = customer_data.loc[customer_id, 'cluster_name']\n",
    "    \n",
    "    print(f\"Customer {customer_id}:\")\n",
    "    print(f\"  Cluster: {cluster_name}\")\n",
    "    print(f\"  Optimal Send Hour: {optimal_hour}:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Frequency Optimization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze optimal email frequency\n",
    "# Group events by customer and count emails\n",
    "emails_per_customer = email_events.groupby('customer_id')['email_id'].nunique()\n",
    "\n",
    "# For each customer, calculate the average time between opens\n",
    "customer_frequency = {}\n",
    "\n",
    "for customer_id in email_events['customer_id'].unique():\n",
    "    customer_events = email_events[email_events['customer_id'] == customer_id].copy()\n",
    "    customer_events = customer_events.sort_values('timestamp')\n",
    "    \n",
    "    # Calculate time between opens\n",
    "    if len(customer_events) > 1:\n",
    "        time_diffs = []\n",
    "        prev_time = None\n",
    "        \n",
    "        for _, event in customer_events.iterrows():\n",
    "            if event['event_type'] == 'email_open':\n",
    "                if prev_time is not None:\n",
    "                    time_diff = (event['timestamp'] - prev_time).total_seconds() / (3600 * 24)  # in days\n",
    "                    # Only consider reasonable time differences (1-30 days)\n",
    "                    if 1 <= time_diff <= 30:\n",
    "                        time_diffs.append(time_diff)\n",
    "                prev_time = event['timestamp']\n",
    "        \n",
    "        if time_diffs:\n",
    "            avg_days = np.mean(time_diffs)\n",
    "            customer_frequency[customer_id] = avg_days\n",
    "\n",
    "# Convert to DataFrame\n",
    "frequency_df = pd.DataFrame(list(customer_frequency.items()), columns=['customer_id', 'optimal_days'])\n",
    "\n",
    "# Plot distribution of optimal frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(frequency_df['optimal_days'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.axvline(frequency_df['optimal_days'].median(), color='red', linestyle='--', label=f\"Median: {frequency_df['optimal_days'].median():.1f} days\")\n",
    "plt.title('Distribution of Optimal Email Frequency')\n",
    "plt.xlabel('Days Between Emails')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Merge frequency data with customer data\n",
    "customer_data = pd.merge(\n",
    "    customer_data,\n",
    "    frequency_df,\n",
    "    left_index=True,\n",
    "    right_on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing frequency data with median\n",
    "median_frequency = frequency_df['optimal_days'].median()\n",
    "customer_data['optimal_days'] = customer_data['optimal_days'].fillna(median_frequency)\n",
    "\n",
    "# Prepare data for training frequency prediction model\n",
    "freq_model_data = customer_data[customer_data['customer_id'].isin(frequency_df['customer_id'])].copy()\n",
    "freq_model_data.set_index('customer_id', inplace=True)\n",
    "\n",
    "# One-hot encode categorical features (if not already done)\n",
    "if 'gender_male' not in freq_model_data.columns:\n",
    "    freq_model_data = pd.get_dummies(\n",
    "        freq_model_data, \n",
    "        columns=['gender', 'location'], \n",
    "        drop_first=True\n",
    "    )\n",
    "    \n",
    "    # Create binary interest indicators\n",
    "    for interest in common_interests:\n",
    "        freq_model_data[f'interest_{interest}'] = freq_model_data['interests'].str.contains(interest).astype(int)\n",
    "\n",
    "# Split features and target for frequency model\n",
    "X_freq = freq_model_data.drop(['cluster', 'cluster_name', 'peak_hour', 'interests', 'optimal_days'], axis=1)\n",
    "y_freq = freq_model_data['optimal_days']\n",
    "\n",
    "# Train a Random Forest model to predict optimal frequency\n",
    "rf_freq_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_freq_model.fit(X_freq, y_freq)\n",
    "\n",
    "# Get feature importances\n",
    "freq_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_freq.columns,\n",
    "    'Importance': rf_freq_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=freq_feature_importance.head(10), palette='viridis')\n",
    "plt.title('Top 10 Features for Predicting Optimal Email Frequency')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to predict optimal email frequency for a customer\n",
    "def predict_optimal_frequency(customer_id, customer_data, rf_freq_model, X_columns):\n",
    "    \"\"\"Predict optimal email frequency for a customer.\"\"\"\n",
    "    if customer_id not in customer_data.index and customer_id not in customer_data['customer_id'].values:\n",
    "        return 7  # Default to weekly if customer not found\n",
    "        \n",
    "    # Get customer data\n",
    "    if customer_id in customer_data.index:\n",
    "        customer = customer_data.loc[customer_id]\n",
    "    else:\n",
    "        customer = customer_data[customer_data['customer_id'] == customer_id].iloc[0]\n",
    "    \n",
    "    # If customer has known optimal frequency, use it\n",
    "    if not pd.isna(customer['optimal_days']):\n",
    "        return customer['optimal_days']\n",
    "        \n",
    "    # Otherwise, use the model to predict\n",
    "    # Prepare customer features (similar to send time prediction)\n",
    "    customer_features = {}\n",
    "    \n",
    "    # Add numeric features\n",
    "    customer_features['age'] = customer['age']\n",
    "    \n",
    "    # Add encoded categorical features\n",
    "    for column in X_columns:\n",
    "        if column.startswith('gender_') or column.startswith('location_'):\n",
    "            # Check if column would be 1 for this customer\n",
    "            if column == f\"gender_{customer['gender']}\" or column == f\"location_{customer['location']}\":\n",
    "                customer_features[column] = 1\n",
    "            else:\n",
    "                customer_features[column] = 0\n",
    "                \n",
    "    # Add interest indicators\n",
    "    interests = customer['interests'].split(',')\n",
    "    for column in X_columns:\n",
    "        if column.startswith('interest_'):\n",
    "            interest = column.replace('interest_', '')\n",
    "            customer_features[column] = 1 if interest in interests else 0\n",
    "    \n",
    "    # Create feature array in correct order\n",
    "    X_customer = pd.DataFrame([customer_features], columns=X_columns)\n",
    "    \n",
    "    # Fill any missing columns with 0\n",
    "    for column in X_columns:\n",
    "        if column not in X_customer.columns:\n",
    "            X_customer[column] = 0\n",
    "    \n",
    "    # Ensure columns are in the right order\n",
    "    X_customer = X_customer[X_columns]\n",
    "    \n",
    "    # Predict optimal frequency\n",
    "    predicted_days = rf_freq_model.predict(X_customer)[0]\n",
    "    \n",
    "    # Round to nearest day and ensure reasonable value (3-14 days)\n",
    "    predicted_days = round(predicted_days)\n",
    "    predicted_days = max(3, min(14, predicted_days))\n",
    "    \n",
    "    return predicted_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the frequency prediction function with a few customers\n",
    "for customer_id in test_customers:\n",
    "    optimal_hour = predict_optimal_send_time(customer_id, customer_data, rf_model, X.columns)\n",
    "    optimal_days = predict_optimal_frequency(customer_id, customer_data, rf_freq_model, X_freq.columns)\n",
    "    cluster_name = customer_data.loc[customer_id, 'cluster_name'] if customer_id in customer_data.index else 'Unknown'\n",
    "    \n",
    "    print(f\"Customer {customer_id}:\")\n",
    "    print(f\"  Cluster: {cluster_name}\")\n",
    "    print(f\"  Optimal Send Hour: {optimal_hour}:00\")\n",
    "    print(f\"  Optimal Frequency: Every {optimal_days:.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with Customer Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze timing patterns by segment\n",
    "# Load segmented customers if available\n",
    "try:\n",
    "    segmented_customers = pd.read_csv('../data/segmented_customers.csv')\n",
    "    \n",
    "    # Merge with timing data\n",
    "    segment_timing = pd.merge(\n",
    "        segmented_customers,\n",
    "        customer_data[['cluster', 'cluster_name', 'peak_hour', 'optimal_days']],\n",
    "        left_on='id',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Analyze timing by segment\n",
    "    segment_timing_avg = segment_timing.groupby('segment').agg({\n",
    "        'peak_hour': 'mean',\n",
    "        'optimal_days': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Plot segment timing patterns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot peak hours by segment\n",
    "    segment_timing_avg['peak_hour'].plot(kind='bar', ax=ax1, color='skyblue')\n",
    "    ax1.set_title('Average Peak Engagement Hour by Segment')\n",
    "    ax1.set_xlabel('Segment')\n",
    "    ax1.set_ylabel('Hour of Day')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot optimal frequency by segment\n",
    "    segment_timing_avg['optimal_days'].plot(kind='bar', ax=ax2, color='lightgreen')\n",
    "    ax2.set_title('Average Optimal Email Frequency by Segment')\n",
    "    ax2.set_xlabel('Segment')\n",
    "    ax2.set_ylabel('Days Between Emails')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Timing recommendations by segment:\")\n",
    "    for segment, row in segment_timing_avg.iterrows():\n",
    "        peak_hour = int(row['peak_hour'])\n",
    "        optimal_days = round(row['optimal_days'])\n",
    "        print(f\"  {segment}: Send at {peak_hour}:00, every {optimal_days} days\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load segmented customers: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Timing Optimization Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamp for model version\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Create package with timing optimization models and data\n",
    "timing_package = {\n",
    "    'time_model': rf_model,\n",
    "    'frequency_model': rf_freq_model,\n",
    "    'time_model_columns': list(X.columns),\n",
    "    'frequency_model_columns': list(X_freq.columns),\n",
    "    'time_clusters': cluster_timing_df.to_dict(),\n",
    "    'time_cluster_names': cluster_names,\n",
    "    'peak_hours': peak_hours,\n",
    "    'default_hour': 12,  # Default to noon\n",
    "    'default_frequency': 7,  # Default to weekly\n",
    "    'version': timestamp,\n",
    "    'time_prediction_function': predict_optimal_send_time,\n",
    "    'frequency_prediction_function': predict_optimal_frequency\n",
    "}\n",
    "\n",
    "# Save the models\n",
    "with open('../data/processed/timing_optimization_models.pkl', 'wb') as f:\n",
    "    pickle.dump(timing_package, f)\n",
    "    \n",
    "print(f\"Saved timing optimization models to '../data/processed/timing_optimization_models.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Timing Optimization API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define API functions for timing optimization\n",
    "\n",
    "def get_optimal_send_time(customer_id, customer_data=None):\n",
    "    \"\"\"API function to get optimal send time for a customer.\"\"\"\n",
    "    # Load models if not provided\n",
    "    if not hasattr(get_optimal_send_time, 'models'):\n",
    "        try:\n",
    "            with open('../data/processed/timing_optimization_models.pkl', 'rb') as f:\n",
    "                get_optimal_send_time.models = pickle.load(f)\n",
    "        except:\n",
    "            return {\n",
    "                \"error\": \"Models not found\",\n",
    "                \"hour\": 12,\n",
    "                \"minute\": 0,\n",
    "                \"day_of_week\": None,\n",
    "                \"confidence\": 0.5\n",
    "            }\n",
    "    \n",
    "    # Use provided customer data or load from database (mock implementation)\n",
    "    if customer_data is None:\n",
    "        try:\n",
    "            # This would be replaced with actual database query\n",
    "            with open('../data/customers.json', 'r') as f:\n",
    "                customers = json.load(f)\n",
    "                customer_found = False\n",
    "                for customer in customers:\n",
    "                    if customer.get('id') == customer_id:\n",
    "                        customer_data = extract_customer_attributes(customer)\n",
    "                        customer_found = True\n",
    "                        break\n",
    "                        \n",
    "                if not customer_found:\n",
    "                    return {\n",
    "                        \"error\": \"Customer not found\",\n",
    "                        \"hour\": 12,\n",
    "                        \"minute\": 0,\n",
    "                        \"day_of_week\": None,\n",
    "                        \"confidence\": 0.5\n",
    "                    }\n",
    "        except:\n",
    "            return {\n",
    "                \"error\": \"Could not load customer data\",\n",
    "                \"hour\": 12,\n",
    "                \"minute\": 0,\n",
    "                \"day_of_week\": None,\n",
    "                \"confidence\": 0.5\n",
    "            }\n",
    "    \n",
    "    # Get models and parameters\n",
    "    models = get_optimal_send_time.models\n",
    "    rf_model = models['time_model']\n",
    "    columns = models['time_model_columns']\n",
    "    \n",
    "    # Predict optimal hour using wrapped prediction function\n",
    "    try:\n",
    "        optimal_hour = predict_optimal_send_time(customer_id, customer_data, rf_model, columns)\n",
    "    except:\n",
    "        optimal_hour = models['default_hour']\n",
    "    \n",
    "    # Use even distribution for minutes (0, 15, 30, 45)\n",
    "    optimal_minute = np.random.choice([0, 15, 30, 45])\n",
    "    \n",
    "    # Get optimal day of week (None means any day is fine)\n",
    "    optimal_day = None\n",
    "    \n",
    "    # Calculate confidence (higher for customers with known patterns)\n",
    "    if customer_id in customer_data.index and customer_data.loc[customer_id, 'cluster'] >= 0:\n",
    "        confidence = 0.9\n",
    "    else:\n",
    "        confidence = 0.7\n",
    "    \n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"hour\": optimal_hour,\n",
    "        \"minute\": optimal_minute,\n",
    "        \"day_of_week\": optimal_day,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "\n",
    "def get_optimal_frequency(customer_id, customer_data=None):\n",
    "    \"\"\"API function to get optimal email frequency for a customer.\"\"\"\n",
    "    # Load models if not provided\n",
    "    if not hasattr(get_optimal_frequency, 'models'):\n",
    "        try:\n",
    "            with open('../data/processed/timing_optimization_models.pkl', 'rb') as f:\n",
    "                get_optimal_frequency.models = pickle.load(f)\n",
    "        except:\n",
    "            return {\n",
    "                \"error\": \"Models not found\",\n",
    "                \"days\": 7,\n",
    "                \"confidence\": 0.5\n",
    "            }\n",
    "    \n",
    "    # Use provided customer data or load from database (mock implementation)\n",
    "    if customer_data is None:\n",
    "        try:\n",
    "            # This would be replaced with actual database query\n",
    "            with open('../data/customers.json', 'r') as f:\n",
    "                customers = json.load(f)\n",
    "                customer_found = False\n",
    "                for customer in customers:\n",
    "                    if customer.get('id') == customer_id:\n",
    "                        customer_data = extract_customer_attributes(customer)\n",
    "                        customer_found = True\n",
    "                        break\n",
    "                        \n",
    "                if not customer_found:\n",
    "                    return {\n",
    "                        \"error\": \"Customer not found\",\n",
    "                        \"days\": 7,\n",
    "                        \"confidence\": 0.5\n",
    "                    }\n",
    "        except:\n",
    "            return {\n",
    "                \"error\": \"Could not load customer data\",\n",
    "                \"days\": 7,\n",
    "                \"confidence\": 0.5\n",
    "            }\n",
    "    \n",
    "    # Get models and parameters\n",
    "    models = get_optimal_frequency.models\n",
    "    rf_freq_model = models['frequency_model']\n",
    "    columns = models['frequency_model_columns']\n",
    "    \n",
    "    # Predict optimal frequency using wrapped prediction function\n",
    "    try:\n",
    "        optimal_days = predict_optimal_frequency(customer_id, customer_data, rf_freq_model, columns)\n",
    "    except:\n",
    "        optimal_days = models['default_frequency']\n",
    "    \n",
    "    # Calculate confidence (higher for customers with known patterns)\n",
    "    if customer_id in frequency_df['customer_id'].values:\n",
    "        confidence = 0.9\n",
    "    else:\n",
    "        confidence = 0.7\n",
    "    \n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"days\": optimal_days,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "\n",
    "# Test API functions\n",
    "print(\"Testing API functions:\")\n",
    "for customer_id in test_customers[:2]:\n",
    "    time_result = get_optimal_send_time(customer_id)\n",
    "    freq_result = get_optimal_frequency(customer_id)\n",
    "    \n",
    "    print(f\"\\nCustomer {customer_id}:\")\n",
    "    print(f\"  Optimal Send Time: {time_result['hour']}:{time_result['minute']:02d}\")\n",
    "    print(f\"  Confidence: {time_result['confidence']:.2f}\")\n",
    "    print(f\"  Optimal Frequency: Every {freq_result['days']} days\")\n",
    "    print(f\"  Confidence: {freq_result['confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate summary of findings\n",
    "print(\"Email Timing Optimization Summary\")\n",
    "print(\"=================================\\n\")\n",
    "\n",
    "print(\"1. Overall Engagement Patterns:\")\n",
    "print(\"   - Highest engagement hours: [Peak hours from hourly_engagement]\")\n",
    "print(\"   - Best days of the week: [Peak days from daily_engagement]\")\n",
    "print(\"   - Average optimal frequency: {:.1f} days\".format(frequency_df['optimal_days'].mean()))\n",
    "print()\n",
    "\n",
    "print(\"2. Customer Timing Clusters:\")\n",
    "for cluster_id, name in cluster_names.items():\n",
    "    cluster_count = (customer_timing_norm['cluster'] == cluster_id).sum()\n",
    "    peak_hour = peak_hours[cluster_id]\n",
    "    print(f\"   - {name} ({cluster_count} customers): Peak engagement at {peak_hour}:00\")\n",
    "print()\n",
    "\n",
    "print(\"3. Implementation Recommendations:\")\n",
    "print(\"   a. Use the trained models to predict optimal send times and frequencies for each customer\")\n",
    "print(\"   b. For new customers without engagement history, use the prediction models based on demographics\")\n",
    "print(\"   c. Update models regularly as more engagement data is collected\")\n",
    "print(\"   d. A/B test different send times to validate and improve model accuracy\")\n",
    "print(\"   e. Consider customer timezone information for global campaigns\")\n",
    "print()\n",
    "\n",
    "print(\"4. Technical Implementation:\")\n",
    "print(\"   a. Exported models are saved in '../data/processed/timing_optimization_models.pkl'\")\n",
    "print(\"   b. API functions are provided for easy integration with email delivery systems\")\n",
    "print(\"   c. Include confidence scores to prioritize high-confidence recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
